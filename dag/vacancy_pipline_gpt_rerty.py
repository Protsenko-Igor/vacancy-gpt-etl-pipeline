# ========== –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ ==========
# 1. Airflow
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.hooks.base import BaseHook
# 2. –î–∞—Ç–∞/–≤—Ä–µ–º—è –∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
from datetime import datetime, timedelta
# 3. Cloud/API
import boto3
from botocore.client import Config
import io
# 4. –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API YandexGPT
import requests
# 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
import json
import pandas as pd
import re
import time

# 6. –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
from collections import Counter

default_args = {
    'owner': 'airflow',
    'retries': 2,
    'retry_delay': timedelta(minutes=3),
}

# ========== –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ ==========
def get_s3_client():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è Yandex Object Storage."""
    conn = BaseHook.get_connection('yandex_object_storage')
    extra_config = conn.extra_dejson
    session = boto3.session.Session()
    return session.client(
        service_name='s3',
        endpoint_url='https://storage.yandexcloud.net',
        aws_access_key_id=extra_config.get('access_key_id'),
        aws_secret_access_key=extra_config.get('secret_access_key'),
        config=Config(s3={'addressing_style': 'virtual'})
    )

# ========== –ó–ê–î–ê–ß–ê 1: –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ==========
def find_files_in_bucket(**kwargs):
    ti = kwargs['ti']
    print("=== –ó–∞–¥–∞—á–∞ 1: –ò—â–µ–º —Ñ–∞–π–ª—ã –≤ –±–∞–∫–µ—Ç–µ ===")

    s3_client = get_s3_client()
    bucket_name = 'n8n-vacancy-bucket'
    folder_prefix = 'vacancies/'

    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)
    files = []
    if 'Contents' in response:
        for obj in response['Contents']:
            if obj['Key'].endswith('.csv'):
                files.append(obj['Key'])
                print(f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {obj['Key']}")

    print(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ CSV-—Ñ–∞–π–ª–æ–≤: {len(files)}")
    ti.xcom_push(key='file_list', value=files)
    ti.xcom_push(key='bucket_name', value=bucket_name)

# ========== –ó–ê–î–ê–ß–ê 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ (—Å –æ—Ç–ø—Ä–∞–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –¥–∞–ª—å—à–µ) ==========
def process_latest_file(**kwargs):
    ti = kwargs['ti']
    print("\n=== –ó–∞–¥–∞—á–∞ 2: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã ===")

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
    files = ti.xcom_pull(task_ids='find_files_in_bucket', key='file_list')
    bucket_name = ti.xcom_pull(task_ids='find_files_in_bucket', key='bucket_name')
    print(f"–ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ files (—Å—ã—Ä–æ–π): {files}")
    print(f"–¢–∏–ø files: {type(files)}")
    print(f"–î–ª–∏–Ω–∞ files: {len(files) if files else 0}")

    if not files:
        print("–û—à–∏–±–∫–∞: –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return

    # 2. –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–µ –ø—É—Ç–∏ –∫ CSV-—Ñ–∞–π–ª–∞–º
    #–ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, –ø–∞–ø–∫–∏ (–æ–∫–∞–Ω—á–∏–≤–∞—é—â–∏–µ—Å—è –Ω–∞ '/') –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–º–µ–Ω–∞
    filtered_files = [f for f in files if f and f.endswith('.csv') and len(f) > 10]
    print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ filtered_files: {filtered_files}")
    print(f"–î–ª–∏–Ω–∞ filtered_files: {len(filtered_files)}")

    if not filtered_files:
        print("–û—à–∏–±–∫–∞: –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö CSV-—Ñ–∞–π–ª–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏!")
        return

    # 3. –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏ –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Ñ–∞–π–ª–æ–≤
    latest_files = sorted(filtered_files)[-4:]  # –ë–µ—Ä—ë–º –¥–≤–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
    print(f"–í—ã–±—Ä–∞–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {latest_files}")
    print(f"–í—ã–±—Ä–∞–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {latest_files}")

    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è DataFrame –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    all_dataframes = []

    # –ì–æ—Ç–æ–≤–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è –∑–∞—Ö–æ–¥–∞ –≤ –±–∞–∫–µ—Ç
    s3_client = get_s3_client()

    # –ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–±–µ—Ä–∞—Ç—å 4 —Ñ–∞–π–ª–∞
    for file_key in latest_files:
        print(f"–ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª: {file_key}")
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏ —á–∏—Ç–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)
        csv_data = obj['Body'].read().decode('utf-8')
        df = pd.read_csv(io.StringIO(csv_data))
        print(f"   -> –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π.")
        all_dataframes.append(df)

    # 2. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ DataFrame –∏–∑ —Å–ø–∏—Å–∫–∞
    if all_dataframes:
        combined_df = pd.concat(all_dataframes, ignore_index=True)
    else:
        combined_df = pd.DataFrame()  # –ü—É—Å—Ç–æ–π DataFrame, –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ –±—ã–ª–æ

    # 3. –£–¥–∞–ª—è–µ–º —è–≤–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (–µ—Å–ª–∏ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ)
    initial_count = len(combined_df)
    combined_df = combined_df.drop_duplicates()
    deduplicated_count = len(combined_df)
    print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {initial_count} –∑–∞–ø–∏—Å–µ–π.")
    print(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {deduplicated_count} –∑–∞–ø–∏—Å–µ–π.")

    # 4. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤–æ–º—É –ø–æ–ª—é, –Ω–∞–ø—Ä–∏–º–µ—Ä 'id'
    combined_df = combined_df.drop_duplicates(subset=['id'])

    # 5. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ—Å—å –∏—Ç–æ–≥–æ–≤—ã–π DataFrame –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    all_vacancies_data = combined_df.to_dict('records')
    print(f"–ò—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ GPT. –°—Ç—Ä–æ–∫: {len(all_vacancies_data)}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: {list(all_vacancies_data[0].keys())}")

    # 6. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –≤ XCom
    ti.xcom_push(key='vacancies_for_gpt', value=all_vacancies_data)

# ========== –ó–ê–î–ê–ß–ê 3: –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ YandexGPT (—Å –±–∞—Ç—á–∏–Ω–≥–æ–º –∏ retry) ==========
def title_with_gpt(**kwargs):
    ti = kwargs['ti']
    print("\n=== –ó–∞–¥–∞—á–∞ 3: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤–∞–∫–∞–Ω—Å–∏–π —á–µ—Ä–µ–∑ GPT ===")

    raw_data = ti.xcom_pull(task_ids='process_latest_file', key='vacancies_for_gpt')
    if not raw_data:
        print("–û—à–∏–±–∫–∞: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return
    
    print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(raw_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.")
    
    # 1. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    all_titles = []
    title_to_records = {}
    
    for idx, record in enumerate(raw_data):
        title = record.get('title', '').strip()
        if title:
            all_titles.append(title)
            if title not in title_to_records:
                title_to_records[title] = []
            title_to_records[title].append(idx)
    
    unique_titles = list(set(all_titles))
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {len(unique_titles)}")
    print(f"–ü—Ä–∏–º–µ—Ä—ã: {unique_titles[:3]}")
    
    # 2. –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á–∏
    conn = BaseHook.get_connection('yandex_gpt')
    api_key = conn.extra_dejson.get('api_key')
    folder_id = conn.extra_dejson.get('folder_id')
    
    # NEW: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ —Å retry
    def process_batch_with_retry(batch_items, max_retries=2):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        
        all_results = []
        current_batch = batch_items.copy()
        
        for attempt in range(max_retries + 1):
            if not current_batch:
                break
                
            print(f"    –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {len(current_batch)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = f"""
            –¢—ã ‚Äî HR-–∞–Ω–∞–ª–∏—Ç–∏–∫, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—à—å –≤–∞–∫–∞–Ω—Å–∏–∏.
            
            –ò—Å—Ö–æ–¥–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è: {', '.join(current_batch)}.

            –ü—Ä–∏–≤–µ–¥–∏ –∫–∞–∂–¥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫ –æ–¥–Ω–æ–π –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:

            - –ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö
            - BI-–∞–Ω–∞–ª–∏—Ç–∏–∫
            - –°–∏—Å—Ç–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫
            - –ë–∏–∑–Ω–µ—Å –∞–Ω–∞–ª–∏—Ç–∏–∫
            - –í–µ–±-–∞–Ω–∞–ª–∏—Ç–∏–∫
            - –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫
            - –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫
            - ML/AI-–∏–Ω–∂–µ–Ω–µ—Ä
            - –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫
            - DevOps-–∏–Ω–∂–µ–Ω–µ—Ä
            - –î–∏—Ä–µ–∫—Ç–æ—Ä –ø–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É
            - –ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä
            - –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –¥–∏—Ä–µ–∫—Ç–æ—Ä
            - –î–∏—Ä–µ–∫—Ç–æ—Ä –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É
            - –ú–∞—Ä–∫–µ—Ç–æ–ª–æ–≥
            - –ì–ª–∞–≤–Ω—ã–π –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥
            - –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
            - –î–∏—Ä–µ–∫—Ç–æ—Ä –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º
            - –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ç—Ä–∞—Ñ–∏–∫—É
            - –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–¥—É–∫—Ç–∞
            - –î—Ä—É–≥–æ–µ
            
            **–ü—Ä–∞–≤–∏–ª–∞**
            1. –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            2. –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω ‚Äî —Å—Ç–∞–≤—å "–î—Ä—É–≥–æ–µ"
            3. –í–∞–∫–∞–Ω—Å–∏–∏ –ø–∏—à–∏ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ)
            4. –ù–ï –¥–æ–±–∞–≤–ª—è–π –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ –ø—Ä–∏–º–µ—Ä–æ–≤.

            –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª—è–º–∏:
            - "original": –∏—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
            - "normalized_title": –≤—ã–±—Ä–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            """
            
            try:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                response = requests.post(
                    url,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Api-Key {api_key}"
                    },
                    json={
                        "modelUri": f"gpt://{folder_id}/yandexgpt-lite/rc",
                        "completionOptions": {
                            "stream": False,
                            "temperature": 0.3,
                            "maxTokens": 4000
                        },
                        "messages": [{"role": "user", "text": prompt}]
                    },
                    timeout=60
                )
                
                print(f"      –°—Ç–∞—Ç—É—Å: {response.status_code}")
                response.raise_for_status()
                
                result = response.json()
                gpt_response_text = result['result']['alternatives'][0]['message']['text']
                print(f"      –û—Ç–≤–µ—Ç: {len(gpt_response_text)} —Å–∏–º–≤–æ–ª–æ–≤")

                
                # –ü–∞—Ä—Å–∏–º JSON
                def safe_json_parse(text):
                    text = text.strip().strip('`')
                    if text.startswith('json'):
                        text = text[4:].strip()
                    
                    try:
                        return json.loads(text)
                    except json.JSONDecodeError:
                        json_match = re.search(r'\[\s*\{.*\}\s*\]', text, re.DOTALL)
                        if json_match:
                            try:
                                return json.loads(json_match.group())
                            except:
                                pass
                        return []
                
                batch_results = safe_json_parse(gpt_response_text)
                
                if not batch_results:
                    print(f"      –ù–µ –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–π JSON")
                    # –°–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –≤—Å–µ–≥–æ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
                    temp_results = []
                    for item in current_batch:
                        temp_results.append({
                            "original": item,
                            "normalized_title": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                        })
                    batch_results = temp_results
                
                # –†–∞–∑–¥–µ–ª—è–µ–º —É—Å–ø–µ—à–Ω—ã–µ –∏ –Ω–µ—É–¥–∞—á–Ω—ã–µ
                successful = []
                failed_items = []
                
                for item in batch_results:
                    if isinstance(item, dict):
                        title = item.get('normalized_title', '')
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                        if title and title != '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞':
                            successful.append(item)
                        else:
                            failed_items.append(item.get('original', ''))
                
                print(f"      –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: {len(successful)}, –ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: {len(failed_items)}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ø–µ—à–Ω—ã–µ –≤ –æ–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                all_results.extend(successful)
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–µ
                current_batch = failed_items
                
                if not failed_items:
                    break  # –í—Å–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –≤—ã—Ö–æ–¥–∏–º
                    
                if attempt < max_retries:
                    print(f"      –ü–∞—É–∑–∞ 2 —Å–µ–∫—É–Ω–¥—ã –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(2)
                
            except Exception as e:
                print(f"      –û—à–∏–±–∫–∞: {e}")
                if attempt == max_retries:
                    # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ –∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –æ—à–∏–±–∫–∞
                    for item in current_batch:
                        all_results.append({
                            "original": item,
                            "normalized_title": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                        })
                    break
                time.sleep(3)  # –ü–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
        # –î–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫
        for item in current_batch:
            all_results.append({
                "original": item,
                "normalized_title": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
            })
        
        return all_results
    
    # 3. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 15 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    batch_size = 15
    all_normalized = []
    
    # URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ (–≤—ã–Ω–æ—Å–∏–º –∏–∑ —Ü–∏–∫–ª–∞)
    url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
    
    total_batches = (len(unique_titles) + batch_size - 1) // batch_size
    print(f"\n=== –ë–∞—Ç—á–∏–Ω–≥ ===")
    print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {len(unique_titles)}")
    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π: {total_batches}")
    
    for batch_num in range(0, len(unique_titles), batch_size):
        batch = unique_titles[batch_num:batch_num + batch_size]
        batch_index = batch_num // batch_size + 1
        
        print(f"\n--- –ë–∞—Ç—á {batch_index}/{total_batches} ---")
        print(f"–ó–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –±–∞—Ç—á–µ: {len(batch)}")
        print(f"–ü—Ä–∏–º–µ—Ä—ã: {batch[:3]}...")
        
        print(f"üëâ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ GPT —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º...")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å retry
            normalized_batch = process_batch_with_retry(
                batch_items=batch,
                max_retries=1  # –û–¥–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
            )
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen = set()
            unique_results = []
            for item in normalized_batch:
                if isinstance(item, dict) and item.get('original'):
                    if item['original'] not in seen:
                        seen.add(item['original'])
                        unique_results.append(item)
            
            all_normalized.extend(unique_results)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞
            success_count = sum(1 for item in unique_results 
                              if item.get('normalized_title') != '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
            print(f"‚úÖ –ò—Ç–æ–≥ –±–∞—Ç—á–∞: {success_count}/{len(batch)} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ #{batch_index} –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫: {e}")
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞
            for title in batch:
                all_normalized.append({
                    "original": title,
                    "normalized_title": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                })
    
    # 4. –°–æ–∑–¥–∞—ë–º –ø–æ–ª–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
    title_mapping = {}
    for item in all_normalized:
        if isinstance(item, dict) and item.get('original'):
            title_mapping[item['original']] = item.get('normalized_title', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
    
    # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –∑–∞–ø–∏—Å—è–º
    enriched_data = []
    for record in raw_data:
        original_title = record.get('title', '').strip()
        enriched_record = record.copy()
        enriched_record['normalized_title'] = title_mapping.get(original_title, '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
        enriched_data.append(enriched_record)
    
    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º
    ti.xcom_push(key='data_with_normalized_titles', value=enriched_data)
    
    # 7. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –ò—Ç–æ–≥–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(enriched_data)}")
    
    from collections import Counter
    normalized_counts = Counter([r['normalized_title'] for r in enriched_data])
    
    print("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for category, count in normalized_counts.most_common(15):
        percentage = (count / len(enriched_data)) * 100
        print(f"  {category}: {count} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)")
    
    # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
    success_count = sum(count for cat, count in normalized_counts.items() 
                       if cat not in ['–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞', '–î—Ä—É–≥–æ–µ'])
    success_rate = (success_count / len(enriched_data)) * 100
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
    undefined_count = normalized_counts.get('–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞', 0)
    if undefined_count > 0:
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞' ({undefined_count} –∑–∞–ø–∏—Å–µ–π):")
        
        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
        undefined_titles = []
        for record in enriched_data[:10]:  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 10
            if record['normalized_title'] == '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞':
                title = record.get('title', '')
                if title:
                    undefined_titles.append(title[:50] + '...' if len(title) > 50 else title)
        
        if undefined_titles:
            print(f"  –ü—Ä–∏–º–µ—Ä—ã: {', '.join(undefined_titles[:5])}")
    
    print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {success_rate:.1f}% –∑–∞–ø–∏—Å–µ–π")
    
    return f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(enriched_data)} –∑–∞–ø–∏—Å–µ–π, —É—Å–ø–µ—Ö: {success_rate:.1f}%"

# ========== –ó–ê–î–ê–ß–ê 4: –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ñ–µ—Ä —á–µ—Ä–µ–∑ YandexGPT (—Å –±–∞—Ç—á–∏–Ω–≥–æ–º –∏ retry) ==========
def working_with_gpt(**kwargs):
    ti = kwargs['ti']
    print("\n=== –ó–∞–¥–∞—á–∞ 4: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ñ–µ—Ä –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ GPT ===")

    # 1. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
    raw_data = ti.xcom_pull(task_ids='title_with_gpt', key='data_with_normalized_titles')
    if not raw_data:
        print("–û—à–∏–±–∫–∞: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return
    
    print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(raw_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.")
    
    # 2. –°–æ–±–∏—Ä–∞–µ–º –í–°–ï –£–ù–ò–ö–ê–õ–¨–ù–´–ï —Å—Ñ–µ—Ä—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    all_fields = []
    field_to_records = {}
    
    for idx, record in enumerate(raw_data):
        field = record.get('ai_field_of_activity', '').strip()
        if field:  # –¢–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ
            all_fields.append(field)
            if field not in field_to_records:
                field_to_records[field] = []
            field_to_records[field].append(idx)
    
    unique_fields = list(set(all_fields))
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ñ–µ—Ä –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {len(unique_fields)}")
    print(f"–ü—Ä–∏–º–µ—Ä—ã: {unique_fields[:3]}")
    
    # 3. –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á–∏
    conn = BaseHook.get_connection('yandex_gpt')
    api_key = conn.extra_dejson.get('api_key')
    folder_id = conn.extra_dejson.get('folder_id')
    
    print(f"API Key –ø–æ–ª—É—á–µ–Ω: {'–î–∞' if api_key else '–ù–µ—Ç'}")
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ —Å retry
    def process_batch_with_retry(batch_items, max_retries=2):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        
        all_results = []
        current_batch = batch_items.copy()
        
        for attempt in range(max_retries + 1):  # +1 –¥–ª—è –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏
            if not current_batch:
                break
                
            print(f"    –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: {len(current_batch)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            sample_working_str = ', '.join(current_batch)
            prompt = f"""
            –¢—ã ‚Äî HR-–∞–Ω–∞–ª–∏—Ç–∏–∫, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—à—å –≤–∞–∫–∞–Ω—Å–∏–∏.
            –ò—Å—Ö–æ–¥–Ω—ã–µ —Å—Ñ–µ—Ä—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {sample_working_str}.

            **–ö–ê–¢–ï–ì–û–†–ò–ò (–≤—ã–±–µ—Ä–∏ –û–î–ù–£):**
            - IT (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞, —Å–æ—Ñ—Ç, saas, ai, it, crm, big data –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –§–∏–Ω–∞–Ω—Å—ã (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –º—Ñ–æ, –±–∞–Ω–∫–∏, –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —É—Å–ª—É–≥–∏, –±–∞–Ω–∫–∏–Ω–≥, —Ñ–∏–Ω—Ç–µ—Ö, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏, —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –†–∏—Ç–µ–π–ª (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: —Ä–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è, FMCG –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - E-commerce (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω—ã, –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã, e-commerce –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å, –∑–∞–≤–æ–¥—ã –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –ú–µ–¥–∏—Ü–∏–Ω–∞ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞ –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: EdTech, –∫—É—Ä—Å—ã, –æ–Ω–ª–∞–π–Ω –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: —Ä–µ–∫–ª–∞–º–∞, digital, –º–µ–¥–∏–∞, cpa –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –õ–æ–≥–∏—Å—Ç–∏–∫–∞ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –¥–æ—Å—Ç–∞–≤–∫–∞, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –¢—É—Ä–∏–∑–º (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è, –≥–æ—Å—Ç–∏–Ω–∏—Ü—ã –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –¢–µ–ª–µ–∫–æ–º (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: —Å–≤—è–∑—å, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ, –∞—Ä–µ–Ω–¥–∞ –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –Ω–µ—Ñ—Ç—å, –≥–∞–∑, —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–µ–∫—Ç–æ—Ä (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –≥–æ—Å—É—Å–ª—É–≥–∏, –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –ø–æ–¥–æ–±–Ω–æ–µ)
            - –ö–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –∫–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥–æ–≤—ã–µ —É—Å–ª—É–≥–∏ –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: –∞–∑–∞—Ä—Ç–Ω—ã–µ –∏–≥—Ä—ã, igaming, gambling –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –°—Ñ–µ—Ä–∞ —É—Å–ª—É–≥ (–µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç: hr, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —É—Å–ª—É–≥–∏ –∏ –ø–æ–¥–æ–±–Ω—ã–µ)
            - –î—Ä—É–≥–æ–µ (–µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –≤—ã—à–µ)

            **–ü–†–ê–í–ò–õ–ê (–≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏ #{attempt + 1}):**
            1. –í—ã–±–µ—Ä–∏ –û–î–ù–£ –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ —Å–ø–∏—Å–∫–∞ –≤—ã—à–µ
            2. –î–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ‚Äî —É–∫–∞–∂–∏ —Å–∞–º–æ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
            3. –ï—Å–ª–∏ —Å–æ–º–Ω–µ–≤–∞–µ—à—å—Å—è ‚Äî —Å—Ç–∞–≤—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é "–î—Ä—É–≥–æ–µ"
            4. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∏—à–∏ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
            5. –ö–æ–≥–¥–∞ —Å–º–æ—Ç—Ä–∏—à—å –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ —Å–∫–æ–±–∫–∞—Ö —É–∫–∞–∑–∞–Ω—ã —É—Å–ª–æ–≤–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –∏—Ö –≤ –æ—Ç–≤–µ—Ç –Ω–µ –Ω—É–∂–Ω–æ)
            5. {'‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–∏ —Å—Ñ–µ—Ä—ã –ù–ï –£–î–ê–õ–û–°–¨ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏! –ë—É–¥—å –±–æ–ª–µ–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–º!' if attempt > 0 else ''}

            **–í–ù–ò–ú–ê–ù–ò–ï:** –ï—Å–ª–∏ —Å—Ñ–µ—Ä–∞ –°–õ–û–ñ–ù–ê–Ø (–Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ "." –∏–ª–∏  "/" ):
            1. –í—ã–±–µ—Ä–∏ –ü–ï–†–í–£–Æ –∏–ª–∏ –û–°–ù–û–í–ù–£–Æ —Å—Ñ–µ—Ä—É
            2. –ò–≥–Ω–æ—Ä–∏—Ä—É–π –≤—Ç–æ—Ä–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–µ
            3. –ï—Å–ª–∏ —Å–æ–º–Ω–µ–≤–∞–µ—à—å—Å—è ‚Äî —Å—Ç–∞–≤—å "–î—Ä—É–≥–æ–µ"
            
            –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª—è–º–∏:
            - "original": –∏—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
            - "category": —à–∏—Ä–æ–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è (—Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã)
            - "specialization": —É–∑–∫–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (—Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã)
            """
            
            try:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                response = requests.post(
                    url,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Api-Key {api_key}"
                    },
                    json={
                        "modelUri": f"gpt://{folder_id}/yandexgpt-lite/rc",
                        "completionOptions": {
                            "stream": False,
                            "temperature": 0.3,
                            "maxTokens": 4000
                        },
                        "messages": [{"role": "user", "text": prompt}]
                    },
                    timeout=60
                )
                
                print(f"      –°—Ç–∞—Ç—É—Å: {response.status_code}")
                response.raise_for_status()
                
                result = response.json()
                gpt_response_text = result['result']['alternatives'][0]['message']['text']
                print(f"      –û—Ç–≤–µ—Ç: {len(gpt_response_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
                def safe_json_parse(text):
                    text = text.strip().strip('`')
                    if text.startswith('json'):
                        text = text[4:].strip()
                    
                    try:
                        return json.loads(text)
                    except json.JSONDecodeError:
                        json_match = re.search(r'\[\s*\{.*\}\s*\]', text, re.DOTALL)
                        if json_match:
                            try:
                                return json.loads(json_match.group())
                            except:
                                pass
                        return []
                
                batch_results = safe_json_parse(gpt_response_text)
                
                if not batch_results:
                    print(f"      –ù–µ –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–π JSON")
                    # –°–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –≤—Å–µ–≥–æ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
                    temp_results = []
                    for item in current_batch:
                        temp_results.append({
                            "original": item,
                            "category": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞",
                            "specialization": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                        })
                    batch_results = temp_results
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö original –µ—Å—Ç—å –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ
                current_batch_set = set(current_batch)
                filtered_results = []
                
                for item in batch_results:
                    if isinstance(item, dict):
                        original = item.get('original', '')
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ original –µ—Å—Ç—å –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ
                        if original in current_batch_set:
                            filtered_results.append(item)
                        else:
                            print(f"      –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á—É–∂–æ–π —ç–ª–µ–º–µ–Ω—Ç: '{original[:50]}...'")
                
                batch_results = filtered_results
                
                if not batch_results:
                    print(f"      –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å 0 –∑–∞–ø–∏—Å–µ–π")
                    # –°–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –≤—Å–µ–≥–æ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
                    temp_results = []
                    for item in current_batch:
                        temp_results.append({
                            "original": item,
                            "category": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞",
                            "specialization": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                        })
                    batch_results = temp_results
                
                # –†–∞–∑–¥–µ–ª—è–µ–º —É—Å–ø–µ—à–Ω—ã–µ –∏ –Ω–µ—É–¥–∞—á–Ω—ã–µ
                successful = []
                failed_items = []
                
                for item in batch_results:
                    if isinstance(item, dict):
                        category = item.get('category', '')
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞" –∏ –Ω–µ "–î—Ä—É–≥–æ–µ"
                        if category and category != '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞' and category != '–î—Ä—É–≥–æ–µ':
                            successful.append(item)
                        else:
                            failed_items.append(item.get('original', ''))
                
                print(f"      –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: {len(successful)}, –ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ: {len(failed_items)}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ø–µ—à–Ω—ã–µ –≤ –æ–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                all_results.extend(successful)
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–µ
                current_batch = failed_items
                
                if not failed_items:
                    break  # –í—Å–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –≤—ã—Ö–æ–¥–∏–º
                    
                if attempt < max_retries:
                    print(f"      –ü–∞—É–∑–∞ 2 —Å–µ–∫—É–Ω–¥—ã –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(2)
                
            except Exception as e:
                print(f"      –û—à–∏–±–∫–∞: {e}")
                if attempt == max_retries:
                    # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ –∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –æ—à–∏–±–∫–∞
                    for item in current_batch:
                        all_results.append({
                            "original": item,
                            "category": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞",
                            "specialization": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                        })
                    break
                time.sleep(3)  # –ü–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
        # –î–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫
        for item in current_batch:
            all_results.append({
                "original": item,
                "category": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞",
                "specialization": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
            })
        
        return all_results
    
    # 4. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 10 —Å—Ñ–µ—Ä
    batch_size = 10
    all_normalized = []
    
    total_batches = (len(unique_fields) + batch_size - 1) // batch_size
    print(f"\n=== –ë–∞—Ç—á–∏–Ω–≥ ===")
    print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ñ–µ—Ä: {len(unique_fields)}")
    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π: {total_batches}")
    
    # URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ (–≤—ã–Ω–æ—Å–∏–º –∏–∑ —Ü–∏–∫–ª–∞)
    url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
    
    for batch_num in range(0, len(unique_fields), batch_size):
        batch = unique_fields[batch_num:batch_num + batch_size]
        batch_index = batch_num // batch_size + 1
        
        print(f"\n--- –ë–∞—Ç—á #{batch_index}/{total_batches} ---")
        print(f"–°—Ñ–µ—Ä –≤ –±–∞—Ç—á–µ: {len(batch)}")
        print(f"–ü—Ä–∏–º–µ—Ä—ã: {batch[:3]}...")
        
        print(f"üëâ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ GPT —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º...")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å retry –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞
            normalized_batch = process_batch_with_retry(
                batch_items=batch,
                max_retries=1  # –û–¥–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
            )
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen = set()
            unique_results = []
            for item in normalized_batch:
                if isinstance(item, dict) and item.get('original'):
                    if item['original'] not in seen:
                        seen.add(item['original'])
                        unique_results.append(item)
            
            all_normalized.extend(unique_results)
            
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Å—á—ë—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            def count_successful_in_batch(results, original_batch):
                """–°—á–∏—Ç–∞–µ—Ç —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –±–∞—Ç—á–∞"""
                original_set = set(original_batch)
                successful = 0
                
                for item in results:
                    if isinstance(item, dict):
                        original = item.get('original', '')
                        category = item.get('category', '')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –Ω–∞—à–µ–≥–æ –±–∞—Ç—á–∞ –∏ –æ–Ω —É—Å–ø–µ—à–µ–Ω
                        if original in original_set and category not in ['–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞', '–î—Ä—É–≥–æ–µ']:
                            successful += 1
                
                return successful
            
            success_count = count_successful_in_batch(unique_results, batch)
            print(f"‚úÖ –ò—Ç–æ–≥ –±–∞—Ç—á–∞: {success_count}/{len(batch)} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ #{batch_index} –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫: {e}")
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞
            for field in batch:
                all_normalized.append({
                    "original": field,
                    "category": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞",
                    "specialization": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                })
    
    # 5. –°–æ–∑–¥–∞—ë–º –ø–æ–ª–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
    category_mapping = {}
    specialization_mapping = {}
    
    for item in all_normalized:
        if isinstance(item, dict) and item.get('original'):
            category_mapping[item['original']] = item.get('category', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
            specialization_mapping[item['original']] = item.get('specialization', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
    
    # 6. –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –∑–∞–ø–∏—Å—è–º
    enriched_data = []
    for record in raw_data:
        original_field = record.get('ai_field_of_activity', '').strip()
        if not original_field:
            original_field = '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
            
        enriched_record = record.copy()
        enriched_record['category'] = category_mapping.get(original_field, '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
        enriched_record['specialization'] = specialization_mapping.get(original_field, '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
        enriched_data.append(enriched_record)
    
    # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    ti.xcom_push(key='data_with_normalized_working', value=enriched_data)
    
    # 8. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n=== –ò—Ç–æ–≥–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ñ–µ—Ä ===")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(enriched_data)}")
    
    from collections import Counter
    category_counts = Counter([r['category'] for r in enriched_data])
    specialization_counts = Counter([r['specialization'] for r in enriched_data])
    
    print(f"\nüìä –®–∏—Ä–æ–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Ç–æ–ø-5):")
    for category, count in category_counts.most_common(5):
        percentage = (count / len(enriched_data)) * 100
        print(f"  {category}: {count} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)")
    
    print(f"\nüéØ –£–∑–∫–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ø-5):")
    for spec, count in specialization_counts.most_common(5):
        percentage = (count / len(enriched_data)) * 100
        print(f"  {spec}: {count} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)")
    
    # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
    success_count = sum(count for cat, count in category_counts.items() 
                       if cat not in ['–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞', '–ù–µ —É–∫–∞–∑–∞–Ω–æ', '–î—Ä—É–≥–æ–µ'])
    success_rate = (success_count / len(enriched_data)) * 100
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    undefined_count = category_counts.get('–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞', 0)
    if undefined_count > 0:
        undefined_examples = []
        for record in enriched_data[:5]:  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 5
            if record['category'] == '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞':
                field = record.get('ai_field_of_activity', '')
                if field:
                    undefined_examples.append(field[:50])
        
        if undefined_examples:
            print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞': {', '.join(undefined_examples)}...")
    
    print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {success_rate:.1f}% ({success_count}/{len(enriched_data)})")
    
    return f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(enriched_data)} –∑–∞–ø–∏—Å–µ–π, —É—Å–ø–µ—Ö: {success_rate:.1f}%"

# ========== –ó–ê–î–ê–ß–ê 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ S3 ==========
def save_enriched_data_to_s3(**kwargs):
    ti = kwargs['ti']
    print("\n=== –ó–∞–¥–∞—á–∞ 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ S3 ===")
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    enriched_data = ti.xcom_pull(task_ids='working_with_gpt', key='data_with_normalized_working')
    if not enriched_data:
        print("–û—à–∏–±–∫–∞: –ù–µ—Ç –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")
        return
    
    print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(enriched_data)} –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    
    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
    df = pd.DataFrame(enriched_data)
    
    # 3. –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    processing_date = datetime.now().strftime('%Y%m%d_%H%M%S')
    df['_processing_date'] = processing_date
    df['_processing_timestamp'] = datetime.now().isoformat()
    
    print(f"–°–æ–∑–¥–∞–Ω DataFrame: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    
    # 4. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ CSV
    csv_buffer = df.to_csv(
        index=False, 
        encoding='utf-8-sig',  # UTF-8 —Å BOM –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        sep=',',               # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        quotechar='"',         # –°–∏–º–≤–æ–ª –∫–∞–≤—ã—á–µ–∫
        escapechar='\\'       # –°–∏–º–≤–æ–ª —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    )
    
    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ S3
    s3_client = get_s3_client()
    bucket_name = 'n8n-vacancy-bucket'
    
    # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    s3_key = f"processed/normalized/vacancies_normalized_{processing_date}.csv"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ S3
    s3_client.put_object(
        Bucket=bucket_name,
        Key=s3_key,
        Body=csv_buffer.encode('utf-8'),
        ContentType='text/csv'
    )
    
    print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ S3: s3://{bucket_name}/{s3_key}")
    print(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(csv_buffer)} –±–∞–π—Ç")
    
    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    ti.xcom_push(key='processed_file_path', value=s3_key)
    ti.xcom_push(key='processed_record_count', value=len(df))
    
    return s3_key

# ========== –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï DAG ==========
with DAG(
    'vacancy_pipline_gpt_rerty',
    default_args=default_args,
    description='–ü–∞–π–ø–ª–∞–π–Ω: –ø–æ–∏—Å–∫, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π —á–µ—Ä–µ–∑ GPT',
    schedule_interval='@daily',
    start_date=datetime(2024, 5, 1),
    catchup=False,
    tags=['portfolio', 'gpt'],
) as dag:

    task_find = PythonOperator(
        task_id='find_files_in_bucket',
        python_callable=find_files_in_bucket,
    )

    task_process = PythonOperator(
        task_id='process_latest_file',
        python_callable=process_latest_file,
    )

    task_title = PythonOperator(
        task_id='title_with_gpt',
        python_callable=title_with_gpt,
    )

    task_working = PythonOperator(
        task_id='working_with_gpt',
        python_callable=working_with_gpt,
    )

    task_save = PythonOperator(
        task_id='save_enriched_data_to_s3',
        python_callable=save_enriched_data_to_s3,
    )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫: task_find -> task_process -> title_with_gpt -> working_with_gpt -> task_load
    task_find >> task_process >> task_title >> task_working  >> task_save